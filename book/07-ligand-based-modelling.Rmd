# Encoding and similarity search of chemical structures {#cheminfo}

```{r setup_chapter7, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=5, fig.width=5)
library(ribiosUtils)
library(ribiosIO)
library(ribiosPlot)
library(openxlsx)
library(tidyverse)
library(ggplot2)
library(emmeans)
library(knitr)
library(rmarkdown)
library(kableExtra)
library(zoo)
theme_set(theme_light(base_size=13))
```


## What do we want to achieve today

* Have a high-level understanding of encoding chemical structures and similarity search
* Know technical terms such as SMILEs, INCHI keys, fingerprint, Tanimoto Index
* Know how Tanimoto Index is applied to fingerprint-based chemical structure search
* Know how graph edit distance is mathematically defined and used to search for similar molecules

## Questions

## Background

### Small molecules

### InChi

### SMILES

### Stereochemistry

### Fingerprints

## Mathematical concepts

### Structure diagrams as graphs

### Similarity measures

Bajusz et al. compared well-known similarity and distance metrics using a large
dataset, and found that the Tanimoto index, Dice index, Cosine coefficient, and
Soergel distance as the best metrics for similarity calculations.

If two chemical structures are represented as an array of bits (bitmaps), $X_i$
is the $i$th bit of $X$, and $\land, \lor$ are bitwise operations *and* and
*or*, respectively. The Tanimoto index is defined as

$$ T_s(X,Y) = \frac{\sum_i (X_i \land Y_i)}{\sum_i (X_i \lor Y_i)}$$

Note that it is essentially equivalent to the Jaccard Index between two sets,
which are defined as the ratio between the size of interaction between two sets
and the size of the union. For sets $A$ and $B$, this can be represented as 

$$ J(A,B) = \frac{|A \cap B|}{|A \cup B|} $$




### Graph edit distance

## Resources for further learning

A short video about basic concepts of drug, drug target, and molecular interactions, https://www.youtube.com/watch?v=u49k72rUdyc
Computational chemistry in drug discovery on youtube, https://www.youtube.com/watch?v=9DESulCWbRQ

# Molecular descriptors and QSAR {#moldes}

## What do we want to achieve today

* Understand what are molecular descriptors and QSARs
* Understand applications of linear models and dimension reduction in the context of QSAR

## Questions

## Background

### Descriptors derived from experimental measurements

### Theoretical molecular descriptors

### Efficacy

### ADMET

## Mathematical concepts and models

### Linear model

```{r fig.height=4, fig.width=6}
library(ribiosPlot)
library(ggplot2)
library(RColorBrewer)
genfun <- function(x) sin(x) + rnorm(length(x), mean=3, sd=0.5)
x <- seq(0, 2*pi, 0.1)
y <- genfun(x)
x2 <- seq(2*pi, 3*pi, 0.1)
y2 <- genfun(x2)
data1 <- data.frame(x=x, y=y)
data2 <- data.frame(x=x2, y=y2)
data1_2 <- rbind(data1, data2)
sinplot <- ggplot(data1, aes(x=x, y=y)) + 
  geom_point() +
  xlim(0, 3*pi) +
  geom_vline(xintercept = 2*pi) +
  geom_point(data=data2, col="gray")
linecols <- brewer.pal(5, "Spectral")
names(linecols) <- c("y~x", sprintf("y~poly(x,%d)",2:5))
sinplot + 
  geom_hline(yintercept=3, col="black") +
  geom_smooth(aes(colour="y~x"), method="lm", formula="y~x", se=FALSE,
              fullrange=TRUE, show.legend=TRUE) +
  geom_smooth(aes(colour="y~poly(x,2)"), method="lm", formula=y~poly(x,2), 
              se=FALSE, fullrange=TRUE) +
  geom_smooth(aes(colour="y~poly(x,3)"), method="lm", formula=y~poly(x,3),  
              se=FALSE, fullrange=TRUE) +
  geom_smooth(aes(colour="y~poly(x,4)"), method="lm", formula=y~poly(x,4), 
              se=FALSE, fullrange=TRUE) +
  geom_smooth(aes(colour="y~poly(x,5)"), method="lm", formula=y~poly(x,5), 
              se=FALSE, fullrange=TRUE) +
  scale_colour_manual(name="Linear models", values=linecols) + 
  guides() +
  ylim(1,5)

```
```{r fig.height=3, fig.width=4}
data1_2_kmeans <- kmeans(data1_2, centers = 3)
kmeansPlot <- ggplot(data1_2 %>% mutate(cluster=factor(data1_2_kmeans$cluster)), 
                     aes(x=x, y=y, col=cluster)) + 
  geom_point() +
  xlim(0, 3*pi)
print(kmeansPlot)
```

Lasso

```{r}
library(glmnet)
library(glmnet)
# Loading the data
data(swiss)
x_vars <- model.matrix(Fertility~. , swiss)[,-1]
y_var <- swiss$Fertility
lambda_seq <- 10^seq(2, -2, by = -.1)
# Splitting the data into test and train
set.seed(86)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
x_test = (-train)
y_test = y_var[x_test]
cv_output <- cv.glmnet(x_vars[train,], y_var[train], relax=TRUE, nfold=5)
# identifying best lamda
best_lam <- cv_output$lambda.min
best_lam
# Output

lasso_best <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = best_lam)
linear <- lm(y~., data=data.frame(y=y_var[train], x_vars[train,]))
linear_pred <- predict(linear, data.frame(x_vars[x_test,]))
glmnet_pred <- predict(lasso_best, s = best_lam, newx = x_vars[x_test,])

final <- data.frame(truth=y_test, glmnet=glmnet_pred[,1], linear=linear_pred) %>%
  pivot_longer(cols=c(glmnet,linear))
# Checking the first six obs
library(ggplot2)
ggplot(final, aes(x=truth, y=value, col=name)) +
  geom_point()

coef(lasso_best)
coef(linear)
```

```{r fig.height=3, fig.width=5}
logistic_data <- data.frame(Hours=c(0.5, 0.75, 1, 1.25, 1.50, 1.75, 1.75,
                                    2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4,
                                    4.25, 4.5, 4.75, 5, 5),
                            Pass=c(0, 0, 0, 0, 0, 0,
                                   1, 0, 1, 0, 1, 0,
                                   1, 0, 1, 1, 1, 1, 1, 1))
ggplot(logistic_data, aes(x=Hours, y=Pass)) +
  geom_point() + 
  geom_smooth(aes(col="linear"), method = "lm", se=FALSE) +
  geom_smooth(aes(col="logistic"), method = "glm", 
              method.args = list(family = "binomial"), 
              se = FALSE) +
  xlab("Hours of preparation") + ylab("Passing the exam") +
  scale_color_manual(name="Model", values=c("linear"="orange", "logistic"="#004495"))
```

$$t = \alpha + \beta x$$
$$y = \frac{1}{1+e^{-t}}$$
$$y = \frac{1}{1+e^{-(\alpha+\beta x)}}$$

```{r nonbi, fig.height=3, fig.width=4}
## an XOR
set.seed(1887)
x <- rep(c(1, 1, -1, -1), each=40) + rnorm(160, sd=0.1)
y <- rep(c(1, -1, -1, 1), each=40) + rnorm(160, sd=0.1)
z <- factor(rep(c(1, 0, 1, 0), each=40))
nonld <- data.frame(index=seq(along=x), x=x, y=y, z=z)
ggplot(data=nonld, aes(x=x, y=y, col=z)) +
  geom_point() +
  ggh4x::coord_axes_inside(labels_inside = TRUE) +
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) 
    
```

```{r}
set.seed(1887)
library(randomForest)
library(tree)
library(caret)
nonld_train <- sample_frac(nonld, 0.8)
nonld_test <- nonld %>% filter(!index %in% nonld_train$index)
rf <- randomForest(z~., data=nonld_train %>% dplyr::select(-index),
                   ntree=100)
nonld_test_pred <- predict(rf, nonld_test)
caret::confusionMatrix(nonld_test_pred, nonld_test$z)
```

```{r nonldTrainTest, fig.height=3, fig.width=7}
nonld_train_test <- rbind(cbind(nonld_train, partition="train"),
                          cbind(nonld_test, partition="test")) %>%
  mutate(partition=factor(partition, levels=c("train", "test")))
ggplot(data=nonld_train_test, aes(x=x, y=y, col=z)) +
  geom_point() + facet_wrap(~partition) +
  ggh4x::coord_axes_inside(labels_inside = TRUE) +
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) 
```

```{r, fig.height=4, fig.width=4}
{
  compactPar()
  plot(rf, main="")
}
```

```{r fig.height=4, fig.width=4}
##install_github('munoztd0/reprtree')
library(reprtree)
reprtree::plot.getTree(rf, k=1)
reprtree::plot.getTree(rf, k=20)
```

```{r}
library(neuralnet)
nonald_train_nn <- nonld_train %>% dplyr::select(-index) %>%
  mutate(z=as.integer(as.character(z)))
nn <- neuralnet(z ~ x + y, data=nonald_train_nn,
                hidden=2, linear.output=TRUE)

nnpred <- compute(nn, nonld_test)$net.result

confusionMatrix(factor(round(nnpred), levels=c(0,1)),
                nonld_test$z)
```

```{r, fig.height=4, fig.width=4}
plot(nn, rep="best")
```

### Unsupervised learning with principal component analysis

### Supervised learning

## Software

pubchem

ChEMBL

https://deepchem.io/

## Resources for further learning

http://www.moleculardescriptors.eu/tutorials/tutorials.htm

# Molecular modelling {#molmod}

## What do we want to achieve today

* Understand the principles of mechanistic modelling in computational chemistry
* Understand the theory of docking, search algorithms, and scoring function
* Understand technical terms including Hooke's law, Force Fields, Lennard-Jones potential
* Know the software AutoDock

## Questions

## Background

### Molecular modelling

### Molecular mechanics and quantum chemistry approaches

### Molecular descriptors

### Force fields

### Docking

The disco analogy

## Mathematical concepts and models

### The Lennard-Jones potential

### Scoring functions for docking

## Software 

Autodock

## Resources for further learning

Neumaier, A. “Molecular Modeling of Proteins and Mathematical Prediction of Protein Structure.” SIAM Review 39, no. 3 (January 1, 1997): 407–60. https://doi.org/10.1137/S0036144594278060.
# Cheminformatics {#chemoinfo}

## What do we want to achieve today

* Have a first impression of common forms of drugs 
* Digital representation of small molecules, in particular SMILEs and InChi keys
* Structure search by fingerprints, Tanimoto coefficient, and graph-edit distances 
* Know information sources of small molecules and drugs, especially ChEBI, CheEMBL, and PubChem

## Questions

## Classification of drugs

Drugs can be classified in different ways, taking different perspectives. From the chemical perspective, for instance, drugs can be classified by their chemical structure. Loosely speaking, we distinguish traditional small-molecule drugs, which are often derived from chemical synthesis, from biologics, which can include recombinant proteins, nucleotides or nucleosides, vaccines, blood products that are used therapeutically (such as immunoglobulins), gene therapy, and cell therapy. 

Small-molecule drugs are sometimes classified by the shared chemical structure, for instance $\beta$-lactam antibiotic (used for bacterial infection), steroid (inflammation and auto-immunity), and benzodiazepine (psychoactive drugs). In the same line, they can be classified by physical chemical properties. For instance, the Biopharmaceutics Classification System classifies drugs by solubility and intestinal permeability.

Besides chemical structures, drugs can be classified by their biological target, or more broadly, by their mechanism of action. Mechanism of action is defined as the modulation of activity of specific biological target(s) that cause the pharmacological effect of the drug. Examples include beta blockers (managing abnormal heart rhythms), angiotensin-converting-enzyme (ACE) inhibitors (high blood pressure and heart failure), and nonsteroidal anti-inflammatory drugs (NSAIDs).

One level above the mechanism of action, which describes biochemical properties, drugs can be classified by a more general biological persepective to describe the anatomical or functional changes they induce. Examples include diuretic (promoting urine production), decongestant (relieving nasal congestion), and bronchodilators (increasing airflow to the lungs). Similarly, drugs can be classified by the therapeutic use, *i.e.* the pathology they are used to treat. For instance, analgesics (pain relief), antiepileptic, or antiviral.

While it is helpful to know the different perspectives underlying the classifications above, most of them are not comprehensive and not amenable for computer queries. Systems have been developed to classify drugs, with prominent examples including the [Anatomical Therapeutic Chemical Classification System](https://www.whocc.no/) (ATC system), and medication codes from [Systematized Nomenclature of Medicine](https://www.hl7.org/fhir/valueset-medication-codes.html) (SNOMAD). ATC and SNOMAD CT are examples of ontologies, which allow annotation, unification, and standarization of terms. See [BioPortal](https://bioportal.bioontology.org/) by the U.S. National Center for Biomedical Ontology for more examples.

## Getting to know two small molecules: caffeine/ valium

## Structural representation of small molecules

### Stereoisomers

Chiral center

The example of Thalidomide

Ofloxacin, levofloxacin and dextrofloxacin are all fluoroquinolone antibiotics. 

* Ofloxacin is a *racemic* (equal) mixture of Levo and Dextro isomers
* Levofloxacin is the more active stereoisomer
* Dextrofloxacin is the less active steroisomer

### SMILES

### InChi key

## Structure search

### Fingerprints

### Tanimoto similarity

### Graph-edit distance

## Structural-activity relationship

## Molecular descriptions of small molecules

### Lipinski's rule of five

## Software and resources

### ChEMBL and ChEBI

### PubChem

### pymol

## Resources for further learning

* The original publication describing the graph edit distance (GED) is [@sanfeliu_distance_1983]. Gao *et al.* [@gao_survey_2010] provided a survey of different ways to determine GED.
* European Molecular Biology Laboratory - European Bioinformatics Institute (EMBL-EBI) provides great training programs. Their online training courses in small molecule and cheminformatics are great resources particular for beginners: https://www.ebi.ac.uk/training/online/topic/small-molecules.
* Companies like NextMove work on improving small-molecule representation and search. Some of their work can be found on SlideShare: https://www.slideshare.net/NextMoveSoftware?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
